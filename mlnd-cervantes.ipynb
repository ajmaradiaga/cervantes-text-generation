{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cervantes 2.0\n",
    "\n",
    "## MLND - Capstone Project\n",
    "\n",
    "![Miguel de Cervantes Saavedra](images/cervantes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Background\n",
    "\n",
    "[Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) is a new area of [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning), which has attracted a lot of attention lately due to the amazing results produced by Deep Learning models. With Deep Learning, it is now possible for an algorithm to predict things, classify images (objects) with great accuracy, detect fraudulent transactions, generate image, sound and text. These are tasks that were previously not possible to achieve by an algorithm and now perform better than a human.\n",
    "\n",
    "In this project we will focus on Text Generation. Text Generation is part of [Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) and can be used to [transcribe speech to text](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf), perform [machine translation](http://arxiv.org/abs/1409.3215), generate handwritten text, image captioning, generate new blog posts or news headlines. \n",
    "\n",
    "RNNs are [very effective](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) when understanding sequence of elements and have been used in the past to generate text. I will use a Recurrent Neural Network to generate text inspired on the works of Cervantes.\n",
    "\n",
    "![Basic RNN -> Unrolled RNN](images/basic_unrolled_RNN.png)\n",
    "\n",
    "In order to generate text, we will look at a class of Neural Network where connections between units form a directed cycle, called Recurrent Neural Network (RNNs). RNNs use an internal memory to process sequences of elements and is able to learn from the syntactic structure of text. Our model will be able to generate text based on the text we train it with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "\n",
    "[Miguel de Cervantes Saavedra](https://en.wikipedia.org/wiki/Miguel_de_Cervantes), was a Spanish writer who is regarded as the greater writer in Spanish language. Famous for his novel, [Don Quixote](https://en.wikipedia.org/wiki/Don_Quixote), considered one of the best fiction novels ever written.\n",
    "\n",
    "Unfortunately, Cervantes passed away 500 years ago and he will not be publishing new novels any time soon…. But, wouldn’t it be great if we could generate some text inspired on Don Quixote and other novels he published?\n",
    "\n",
    "To solve our problem, we can use text from novels written by Cervantes in combination with the incredible power of Deep Learning, in particular RNNs, to generate text. Our deep learning model will be trained on existing Cervantes works and will output new text, based on the internal representation of the text it was trained on, in the Neural Network.  \n",
    "\n",
    "![LSTM Cell](images/lstm_cell.png)\n",
    "\n",
    "LSTM Cell\n",
    "\n",
    "For our model to learn, we will use a special type of RNN called LSTMs (Long Short Term Memory), capable of learning long-term dependencies. LSTM can use its memory to generate complex, [realistic sequences](https://arxiv.org/pdf/1308.0850.pdf) containing long-range structure, just like the sentences that we want to generate. It will be able to remember information for a period of time, which will help at generating text of better quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we will use the text from his most famous novel (Don Quixote) and other [less known](http://www.gutenberg.org/cache/epub/14420/pg14420.txt) like Lady Cornelia, The Deceitful Marriage, The Little Gipsy Girl, etc. Also, we will not include any Plays, e.g. [Numancia](https://en.wikipedia.org/wiki/Miguel_de_Cervantes#La_Numancia), to train our model as it’s writing style differs from the novels and we want the generated text to follow the structure of a novel. All the novels are no longer protected under copyright and thanks to the [Gutenberg Project](https://www.gutenberg.org/), we are able to access all the text of [these books](https://www.gutenberg.org/ebooks/author/505).\n",
    "\n",
    "Even though Miguel de Cervantes native language was Spanish, the text used to train our model will be in English. This is to make it easier for the reader to understand the input and output of our model.\n",
    "\n",
    "Our Dataset is small as it is composed of only 2 files - Don Quixote and Exemplary Novels with a total size of 3.4 MB. Bigger datasets work better when training an RNN but for our case that is very specific it will be enough. Some additional information of the contents of the files below:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>**File**</td>\n",
    "    <td></td>\n",
    "    <td>**Totals**</td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*Name*</td>\n",
    "    <td>*Size*</td>\n",
    "    <td>*Pages*</td>\n",
    "    <td>*Lines*</td>\n",
    "    <td>*Words*</td>\n",
    "    <td>*Unique Words*</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DonQuixote.txt</td>\n",
    "    <td>2.3 MB</td>\n",
    "    <td>690</td>\n",
    "    <td>40,008</td>\n",
    "    <td>429,256</td>\n",
    "    <td>42154</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ExemplaryNovels.txt</td>\n",
    "    <td>1.1 MB</td>\n",
    "    <td>303</td>\n",
    "    <td>17,572</td>\n",
    "    <td>189,037</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "* Note: Values in the table above will change after preprocessing.\n",
    "\n",
    "There is some manual preprocessing that we will need to do as the text retrieved from Gutenberg Project contains additional content that is not necessary to train the model, for example:\n",
    "\n",
    "* Preface\n",
    "* Translator’s Preface\n",
    "* About the author\n",
    "* Index\n",
    "* Dedications\n",
    "* Footnotes includes in Exemplary Novels\n",
    "\n",
    "**Note:** The files included in the dataset folder no longer contain the additional content mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Lets start by loading our Data and exploring it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "filenames = [\"dataset/DonQuixote.txt\", \"dataset/ExemplaryNovels.txt\"]\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for fn in filenames:\n",
    "    with open(fn, \"r\") as f:\n",
    "            text += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Unique words: 39229\n",
      "Number of chapters: 135\n",
      "Average number of sentences in each chapters: 392.7925925925926\n",
      "Number of lines: 53162\n",
      "Average number of words in each line: 10.975941461946503\n"
     ]
    }
   ],
   "source": [
    "print('Dataset Stats')\n",
    "print('Unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "chapters = text.split('\\n\\n\\n\\n')\n",
    "print('Number of chapters: {}'.format(len(chapters)))\n",
    "sentence_count_chapter = [chapter.count('\\n') for chapter in chapters]\n",
    "print('Average number of sentences in each chapters: {}'.format(np.average(sentence_count_chapter)))\n",
    "\n",
    "sentences = [sentence for chapter in chapters for sentence in chapter.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Preprocessing \n",
    "We need to prepare our data for our RNN, lets do some additional preprocessing:\n",
    "* Lookup table: We need to create [word embeddings](https://www.tensorflow.org/tutorials/word2vec#motivation_why_learn_word_embeddings) to facilitate the training of our model. \n",
    "\n",
    "* Tokenize punctuation: This is to simplify training for our neural network. Making it easy for it to distinguish between *mad* and *mad!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of dataset split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    \n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {v:k for k, v in vocab_to_int.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"mad\" and \"mad!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lookup = {\".\": \"||period||\", \\\n",
    "         \",\": \"||comma||\", \\\n",
    "         '\"': \"||quotation_mark||\", \\\n",
    "         \";\": \"||semicolon||\", \\\n",
    "         \"!\": \"||exclamation_mark||\", \\\n",
    "         \"?\": \"||question_mark||\", \\\n",
    "         \"(\": \"||l_parenthesis||\", \\\n",
    "         \")\": \"||r_parenthesis||\", \\\n",
    "         \"--\": \"||dash||\", \\\n",
    "         \"\\n\": \"||return||\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for key, token in token_lookup.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "text = text.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "\n",
    "# Saving the preprocessed data\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_lookup), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Check Point\n",
    "The preprocessed data has been saved to disk. No need to preprocess it again, by running the cell below it will be available to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cervantes Neural Network\n",
    "Before getting started, lets check some requirements to run the Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "\n",
    "A GPU is suggested to train the Cervantes Neural Network as text generation takes a long time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network as text generation takes a long time to train in order to achieve good results.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Code\n",
    "The building blocks of the Cervantes Neural Network are include in cervantes_nn.py. If you want to view the code run *cervnn??* in a separate cell after importing it.\n",
    "\n",
    "Functions included in cervantes_nn:\n",
    "- get_inputs: Creates the TF Placeholders for the Neural Network\n",
    "- get_init_cell: Creates our RNN cell and initialises it.\n",
    "- get_embed: Applies [embedding](https://www.tensorflow.org/tutorials/word2vec) to our input data.\n",
    "- build_rnn: Creates a RNN using a RNN cell\n",
    "- build_nn: Apply embedding to input data using your get_embed function. Builds RNN using cell and the build_rnn function. Finally, it applies a [fully connected layer](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected) with a linear activation.\n",
    "- get_batches: Creates a generator that returns batches of data used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cervants_nn as cervnn\n",
    "\n",
    "cervnn.reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View the code of cervantes_nn\n",
    "cervnn??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cervantes Neural Network Training\n",
    "### Hyperparameters\n",
    "The following parameters are used to tune the Neural Network:\n",
    "\n",
    "- `batch_size`: The number of training examples in one pass.\n",
    "- `num_epochs`: One pass of all the training examples.\n",
    "- `rnn_layer_size`: Number of RNN layers\n",
    "- `rnn_size`: Size of the RNNs.\n",
    "- `embed_dim`: Size of the embedding.\n",
    "- `seq_length`: Number of words included in every sequence, e.g. sequence of five words. \n",
    "- `learning_rate`: How fast/slow the Neural Network will train.\n",
    "- `dropout`: Simple way to prevents an RNN from overfitting - [link](http://jmlr.org/papers/v15/srivastava14a.html).\n",
    "- `show_every_n_batches`: Number of batches the neural network should print progress.\n",
    "- `save_every_n_epochs`: Number of epochs the neural network should save progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 512\n",
    "# Number of Epochs\n",
    "num_epochs = 500\n",
    "# RNN Layers\n",
    "rnn_layer_size = 2\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "# Using 300 as it is commonly used in Google's news word vectors and the GloVe vectors\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 20\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout\n",
    "dropout = 0.6\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "# Save progress for every n number of epochs\n",
    "save_every_n_epochs = 100\n",
    "\n",
    "# Define saving directories\n",
    "save_dir = './checkpoints/save'\n",
    "logs_dir = './logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using Cervantes neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Layers: 2 and Size: 256, Batch Size: Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "(?, ?, 300)\n",
      "RNN Layers: 2 and Size: 256, Batch Size: 512\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # Inputs\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = cervnn.get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    \n",
    "    # Define the RNN cell\n",
    "    cell, initial_state = cervnn.get_init_cell(batch_size=input_data_shape[0], \n",
    "                                               rnn_layers=rnn_layer_size, \n",
    "                                               rnn_size=rnn_size,\n",
    "                                               keep_prob=dropout)\n",
    "    # Builds Neural Network\n",
    "    logits, final_state = cervnn.build_nn(cell, input_text, vocab_size, embed_dim,\n",
    "                                         batch_size, rnn_layer_size, rnn_size, dropout)\n",
    "\n",
    "    \n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train Cervantes neural network on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/70   train_loss = 9.989\n",
      "Epoch   1 Batch   30/70   train_loss = 6.203\n",
      "Epoch   2 Batch   60/70   train_loss = 6.203\n",
      "Epoch   4 Batch   20/70   train_loss = 6.171\n",
      "Epoch   5 Batch   50/70   train_loss = 6.084\n",
      "Epoch   7 Batch   10/70   train_loss = 5.993\n",
      "Epoch   8 Batch   40/70   train_loss = 5.923\n",
      "Epoch  10 Batch    0/70   train_loss = 5.844\n",
      "Epoch  11 Batch   30/70   train_loss = 5.831\n",
      "Epoch  12 Batch   60/70   train_loss = 5.772\n",
      "Epoch  14 Batch   20/70   train_loss = 5.714\n",
      "Epoch  15 Batch   50/70   train_loss = 5.690\n",
      "Epoch  17 Batch   10/70   train_loss = 5.619\n",
      "Epoch  18 Batch   40/70   train_loss = 5.558\n",
      "Epoch  20 Batch    0/70   train_loss = 5.482\n",
      "Epoch  21 Batch   30/70   train_loss = 5.498\n",
      "Epoch  22 Batch   60/70   train_loss = 5.468\n",
      "Epoch  24 Batch   20/70   train_loss = 5.428\n",
      "Epoch  25 Batch   50/70   train_loss = 5.407\n",
      "Epoch  27 Batch   10/70   train_loss = 5.350\n",
      "Epoch  28 Batch   40/70   train_loss = 5.297\n",
      "Epoch  30 Batch    0/70   train_loss = 5.226\n",
      "Epoch  31 Batch   30/70   train_loss = 5.246\n",
      "Epoch  32 Batch   60/70   train_loss = 5.215\n",
      "Epoch  34 Batch   20/70   train_loss = 5.188\n",
      "Epoch  35 Batch   50/70   train_loss = 5.161\n",
      "Epoch  37 Batch   10/70   train_loss = 5.116\n",
      "Epoch  38 Batch   40/70   train_loss = 5.043\n",
      "Epoch  40 Batch    0/70   train_loss = 4.973\n",
      "Epoch  41 Batch   30/70   train_loss = 4.985\n",
      "Epoch  42 Batch   60/70   train_loss = 4.945\n",
      "Epoch  44 Batch   20/70   train_loss = 4.883\n",
      "Epoch  45 Batch   50/70   train_loss = 4.830\n",
      "Epoch  47 Batch   10/70   train_loss = 4.784\n",
      "Epoch  48 Batch   40/70   train_loss = 4.702\n",
      "Epoch  50 Batch    0/70   train_loss = 4.624\n",
      "Epoch  51 Batch   30/70   train_loss = 4.596\n",
      "Epoch  52 Batch   60/70   train_loss = 4.570\n",
      "Epoch  54 Batch   20/70   train_loss = 4.518\n",
      "Epoch  55 Batch   50/70   train_loss = 4.484\n",
      "Epoch  57 Batch   10/70   train_loss = 4.429\n",
      "Epoch  58 Batch   40/70   train_loss = 4.372\n",
      "Epoch  60 Batch    0/70   train_loss = 4.286\n",
      "Epoch  61 Batch   30/70   train_loss = 4.308\n",
      "Epoch  62 Batch   60/70   train_loss = 4.250\n",
      "Epoch  64 Batch   20/70   train_loss = 4.298\n",
      "Epoch  65 Batch   50/70   train_loss = 4.200\n",
      "Epoch  67 Batch   10/70   train_loss = 4.161\n",
      "Epoch  68 Batch   40/70   train_loss = 4.123\n",
      "Epoch  70 Batch    0/70   train_loss = 4.032\n",
      "Epoch  71 Batch   30/70   train_loss = 4.060\n",
      "Epoch  72 Batch   60/70   train_loss = 3.987\n",
      "Epoch  74 Batch   20/70   train_loss = 3.997\n",
      "Epoch  75 Batch   50/70   train_loss = 3.984\n",
      "Epoch  77 Batch   10/70   train_loss = 3.898\n",
      "Epoch  78 Batch   40/70   train_loss = 3.903\n",
      "Epoch  80 Batch    0/70   train_loss = 3.792\n",
      "Epoch  81 Batch   30/70   train_loss = 3.827\n",
      "Epoch  82 Batch   60/70   train_loss = 3.777\n",
      "Epoch  84 Batch   20/70   train_loss = 3.776\n",
      "Epoch  85 Batch   50/70   train_loss = 3.740\n",
      "Epoch  87 Batch   10/70   train_loss = 3.685\n",
      "Epoch  88 Batch   40/70   train_loss = 3.715\n",
      "Epoch  90 Batch    0/70   train_loss = 3.615\n",
      "Epoch  91 Batch   30/70   train_loss = 3.632\n",
      "Epoch  92 Batch   60/70   train_loss = 3.575\n",
      "Epoch  94 Batch   20/70   train_loss = 3.581\n",
      "Epoch  95 Batch   50/70   train_loss = 3.541\n",
      "Epoch  97 Batch   10/70   train_loss = 3.508\n",
      "Epoch  98 Batch   40/70   train_loss = 3.524\n",
      "Epoch 100 Batch    0/70   train_loss = 3.418\n",
      "Epoch 101 Batch   30/70   train_loss = 3.488\n",
      "Epoch 102 Batch   60/70   train_loss = 3.456\n",
      "Epoch 104 Batch   20/70   train_loss = 3.405\n",
      "Epoch 105 Batch   50/70   train_loss = 3.379\n",
      "Epoch 107 Batch   10/70   train_loss = 3.357\n",
      "Epoch 108 Batch   40/70   train_loss = 3.380\n",
      "Epoch 110 Batch    0/70   train_loss = 3.323\n",
      "Epoch 111 Batch   30/70   train_loss = 3.305\n",
      "Epoch 112 Batch   60/70   train_loss = 3.271\n",
      "Epoch 114 Batch   20/70   train_loss = 3.248\n",
      "Epoch 115 Batch   50/70   train_loss = 3.229\n",
      "Epoch 117 Batch   10/70   train_loss = 3.198\n",
      "Epoch 118 Batch   40/70   train_loss = 3.239\n",
      "Epoch 120 Batch    0/70   train_loss = 3.166\n",
      "Epoch 121 Batch   30/70   train_loss = 3.155\n",
      "Epoch 122 Batch   60/70   train_loss = 3.142\n",
      "Epoch 124 Batch   20/70   train_loss = 3.117\n",
      "Epoch 125 Batch   50/70   train_loss = 3.098\n",
      "Epoch 127 Batch   10/70   train_loss = 3.141\n",
      "Epoch 128 Batch   40/70   train_loss = 3.097\n",
      "Epoch 130 Batch    0/70   train_loss = 3.063\n",
      "Epoch 131 Batch   30/70   train_loss = 3.082\n",
      "Epoch 132 Batch   60/70   train_loss = 3.028\n",
      "Epoch 134 Batch   20/70   train_loss = 3.017\n",
      "Epoch 135 Batch   50/70   train_loss = 2.979\n",
      "Epoch 137 Batch   10/70   train_loss = 2.998\n",
      "Epoch 138 Batch   40/70   train_loss = 3.025\n",
      "Epoch 140 Batch    0/70   train_loss = 2.934\n",
      "Epoch 141 Batch   30/70   train_loss = 2.919\n",
      "Epoch 142 Batch   60/70   train_loss = 2.907\n",
      "Epoch 144 Batch   20/70   train_loss = 2.948\n",
      "Epoch 145 Batch   50/70   train_loss = 2.883\n",
      "Epoch 147 Batch   10/70   train_loss = 2.939\n",
      "Epoch 148 Batch   40/70   train_loss = 2.922\n",
      "Epoch 150 Batch    0/70   train_loss = 2.894\n",
      "Epoch 151 Batch   30/70   train_loss = 2.865\n",
      "Epoch 152 Batch   60/70   train_loss = 2.814\n",
      "Epoch 154 Batch   20/70   train_loss = 2.814\n",
      "Epoch 155 Batch   50/70   train_loss = 2.776\n",
      "Epoch 157 Batch   10/70   train_loss = 2.787\n",
      "Epoch 158 Batch   40/70   train_loss = 2.765\n",
      "Epoch 160 Batch    0/70   train_loss = 2.739\n",
      "Epoch 161 Batch   30/70   train_loss = 2.751\n",
      "Epoch 162 Batch   60/70   train_loss = 2.705\n",
      "Epoch 164 Batch   20/70   train_loss = 2.708\n",
      "Epoch 165 Batch   50/70   train_loss = 2.680\n",
      "Epoch 167 Batch   10/70   train_loss = 2.670\n",
      "Epoch 168 Batch   40/70   train_loss = 2.727\n",
      "Epoch 170 Batch    0/70   train_loss = 2.679\n",
      "Epoch 171 Batch   30/70   train_loss = 2.632\n",
      "Epoch 172 Batch   60/70   train_loss = 2.607\n",
      "Epoch 174 Batch   20/70   train_loss = 2.609\n",
      "Epoch 175 Batch   50/70   train_loss = 2.624\n",
      "Epoch 177 Batch   10/70   train_loss = 2.588\n",
      "Epoch 178 Batch   40/70   train_loss = 2.600\n",
      "Epoch 180 Batch    0/70   train_loss = 2.599\n",
      "Epoch 181 Batch   30/70   train_loss = 2.547\n",
      "Epoch 182 Batch   60/70   train_loss = 2.511\n",
      "Epoch 184 Batch   20/70   train_loss = 2.518\n",
      "Epoch 185 Batch   50/70   train_loss = 2.525\n",
      "Epoch 187 Batch   10/70   train_loss = 2.525\n",
      "Epoch 188 Batch   40/70   train_loss = 2.540\n",
      "Epoch 190 Batch    0/70   train_loss = 2.486\n",
      "Epoch 191 Batch   30/70   train_loss = 2.494\n",
      "Epoch 192 Batch   60/70   train_loss = 2.462\n",
      "Epoch 194 Batch   20/70   train_loss = 2.473\n",
      "Epoch 195 Batch   50/70   train_loss = 2.429\n",
      "Epoch 197 Batch   10/70   train_loss = 2.492\n",
      "Epoch 198 Batch   40/70   train_loss = 2.582\n",
      "Epoch 200 Batch    0/70   train_loss = 2.459\n",
      "Epoch 201 Batch   30/70   train_loss = 2.406\n",
      "Epoch 202 Batch   60/70   train_loss = 2.355\n",
      "Epoch 204 Batch   20/70   train_loss = 2.390\n",
      "Epoch 205 Batch   50/70   train_loss = 2.476\n",
      "Epoch 207 Batch   10/70   train_loss = 2.400\n",
      "Epoch 208 Batch   40/70   train_loss = 2.452\n",
      "Epoch 210 Batch    0/70   train_loss = 2.432\n",
      "Epoch 211 Batch   30/70   train_loss = 2.341\n",
      "Epoch 212 Batch   60/70   train_loss = 2.296\n"
     ]
    }
   ],
   "source": [
    "batches = cervnn.get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "# file_name_suffix = \"-lr-{}-epochs-{}-sqe_length-{}-\".format(learning_rate, num_epochs, seq_length)\n",
    "run_id = '0004'\n",
    "\n",
    "training_log = \"batch_size: {}\\nepochs: {}\\nrnn_layer_size: {}\\nrnn_size: {}\\nembed_dim: {}\\nseq_length: {}\\nlr: {}\\ndropout: {}\\n--------\\n\".format(batch_size, num_epochs, rnn_layer_size, rnn_size, embed_dim, seq_length, learning_rate, dropout)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                current_log = 'Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss)\n",
    "                training_log += current_log + \"\\n\"\n",
    "                print(current_log)\n",
    "                \n",
    "                # Save every 100 epochs\n",
    "                if (epoch_i + 1) % save_every_n_epochs == 0:\n",
    "                    saver = tf.train.Saver()\n",
    "                    saver.save(sess, save_dir + '-' + run_id + '--c_epoch-' + str(epoch_i + 1))\n",
    "                    model_saved_msg = 'Model Trained and Saved - Epoch: ' + str(epoch_i + 1)\n",
    "                    print(model_saved_msg)\n",
    "                    training_log += model_saved_msg + \"\\n\"\n",
    "                \n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')\n",
    "    \n",
    "    text_file = open(logs_dir + \"training_log-{}.txt\".format(run_id), \"w\")\n",
    "    text_file.write(training_log)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new Cervantes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "pickle.dump((seq_length, save_dir), open('params-{}.p'.format(run_id), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training results\n",
    "\n",
    "The table below captures the results of training the Cervantes Neural Network with different hyperparameters:\n",
    "\n",
    "| Run ID | Batch Size | Epochs | RNN Layers | RNN Size | Embed Dim | Seq Length | LR | Dropout | Train Loss |\n",
    "|:---:|:---:|:---:|:---:|:----:|:----:|:----:|:----:|:-----:|\n",
    "| 0001 | 512 | 300 | 2 | 256 | 300 | 5 | 0.01 | 0.6 | 3.438 |\n",
    "| 0002 | 512 | 500 | 2 | 256 | 500 | 5 | 0.001 | 0.6 | 1.488 |\n",
    "| 0003 | 512 | 300 | 2 | 256 | 300 | 10 | 0.01 | 0.6 | 3.015 |\n",
    "| 0004 | 512 | 500 | 2 | 256 | 500 | 10 | 0.001 | 0.6 | 1.220 |\n",
    "| 0005 | 512 | 500 | 2 | 256 | 500 | 20 | 0.001 | 0.6 | 1.220 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cervantes Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before generating text, lets import our preprocessed data and the params of our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "run_id = \"0004\"\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, meta_dir = pickle.load(open('params-{}.p'.format(run_id), mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below will generate Cervantes text based on some input.\n",
    "- `load_dir`: Location where the graph metadata is saved\n",
    "- `prime_word`: First word used to generate text\n",
    "- `gen_length`: Length of text we want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # Adding randomness to the word returned\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n",
    "    #return int_to_vocab[np.argmax(probabilities)]\n",
    "\n",
    "def generate_text(load_dir, prime_word, gen_length):\n",
    "    \"\"\"\n",
    "    Generates text\n",
    "    :param load_dir: Location where the graph metadata is saved\n",
    "    :param prime_word: First word used to generate text\n",
    "    :param gen_length: How long the generated text will be\n",
    "    :return: Generated text\n",
    "    \"\"\"\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        input_text, initial_state, final_state, probs = cervnn.get_tensors(loaded_graph)\n",
    "\n",
    "        # Sentences generation setup\n",
    "        gen_sentences = [prime_word]\n",
    "        prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "        # Generate sentences\n",
    "        for n in range(gen_length):\n",
    "            # Dynamic Input\n",
    "            dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "            dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "            # Get Prediction\n",
    "            probabilities, prev_state = sess.run(\n",
    "                [probs, final_state],\n",
    "                {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "            pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "            gen_sentences.append(pred_word)\n",
    "\n",
    "        # Remove tokens\n",
    "        generated_text = ' '.join(gen_sentences)\n",
    "        for key, token in token_dict.items():\n",
    "            ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "            generated_text = generated_text.replace(' ' + token.lower(), key)\n",
    "        generated_text = generated_text.replace('\\n ', '\\n')\n",
    "        generated_text = generated_text.replace('( ', '(')\n",
    "\n",
    "        return generated_text\n",
    "    \n",
    "def print_text_for(run_id, epochs, initial_word, initial_epoch=100, text_length=100):\n",
    "    for epoch in range(initial_epoch, epochs + 100, 100):\n",
    "        print('-----------\\n{} at run_id: {}, epoch: {}, text generated: \\n------------\\n{}'.format(initial_word, run_id, epoch, generate_text(meta_dir + '-' + run_id + '--c_epoch-' + str(epoch), initial_word, text_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start comparing text generated by our Cervantes Neural Networks with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 0001\n",
    "\n",
    "print_text_for(run_id='0001', epochs=300, initial_word=\"Quixote\", initial_epoch=300, text_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_text_for(run_id='0004', epochs=500, initial_word=\"Quixote\", initial_epoch=500, , text_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print_text_for(run_id='0005', epochs=500, initial_word=\"Quixote\", initial_epoch=500, , text_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Sancho at run_id: 0001, epoch: 100, text generated: \n",
      "------------\n",
      "Sancho--\" I should do thou king in aught!\" said he,\" give me so well for the least frequented, now\n",
      "did you; nevertheless say to follow a flight without rank and injurious, or some his servant's head, wit that ring had been provoked; and\n",
      "Sancho caught out, and\n",
      "the world stand, incessant doubtful\n",
      "they meet him,\n",
      "commending himself up and Christians; and everyone nor is without speaking sorely discomfited, and the stranger asked Don Quixote of spilling the inn mentioned the\n",
      "-----------\n",
      "Sancho at run_id: 0001, epoch: 200, text generated: \n",
      "------------\n",
      "Sancho, for when for thy persons approaching and had made herself passed at the capture, and blessing\n",
      "back being astonished; and with what I now must bring you other\n",
      "difference other gentleman) invited him to go, on desolate heaths\n",
      "me of my own tastes?\" Startled so we,\n",
      "waiting got applying how thou hast.\" She\n",
      "accepts his grandeur, with no\n",
      "that respect, sprightly or neither, made the same force with him, and\n",
      "with great hopes of age to Anselmo.\"\n",
      "-----------\n",
      "Sancho at run_id: 0001, epoch: 300, text generated: \n",
      "------------\n",
      "Sancho, this day,\n",
      "I drew in this castle. So please the night and I may add, performing with those directed themselves. She has any other chance Master Elisabad and that the\n",
      "order of here, unfortunate as I am, moreover,\n",
      "\n",
      "Master father, that Don Juan was under such as they asked the fuel of doing\n",
      "to which can be so much required before\n",
      "everything of Alcobendas, is\n",
      "will seem a stone or foot; I feel certainly, by nothing, there are none of\n"
     ]
    }
   ],
   "source": [
    "print_text_for(run_id='0001', epochs=300, initial_word=\"Sancho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Sancho at run_id: 0001, epoch: 100, text generated: \n",
      "------------\n",
      "Sancho;\" be seen it had fallen in pieces, and is as yet I called another, brother, and my sister knew that there,\n",
      "Wearied the duchess, and in the people of us to\n",
      "ransom Preciosa, and mistress of Rodaja had roses of thrashing, or injurious to love\n",
      "reaches journey, time for each one beast, for thou art by my entire man, for you are sure will.\n",
      "\n",
      "\" Many that are we may be able to think.\"\n",
      "\n",
      "\" A discerning man\n",
      "-----------\n",
      "Sancho at run_id: 0001, epoch: 200, text generated: \n",
      "------------\n",
      "Sancho\n",
      "Panza, whom, to all\n",
      "my grave, that the\n",
      "book of justice a trifling countenance as of Dorothea, and believe are that they ought to make\n",
      "chance together, and had made so piteous taken countess, or screened by spending them. The curate made with his only orders\n",
      "were reviving and other bowels of\n",
      "linen,\n",
      "in order from eight loaves of yours of short of us never have you\n",
      "dare to look to attack the princess to go before at their huts to this country,\n",
      "-----------\n",
      "Sancho at run_id: 0001, epoch: 300, text generated: \n",
      "------------\n",
      "Sancho?\" said Don Quixote.\n",
      "\n",
      "\" Le night to be,\" said Sancho.\n",
      "\n",
      "\" I do not have given me, and his master and mistress gave me to\n",
      "know by her father, or not to encounter us enough, for he eats himself, and God sent me, however,\" replied Don Juan,\" Thou hast said in such\n",
      "affirmative; and when they\n",
      "told them just he had said,\" Who goes from it is\n",
      "the dyed of Africa; nor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_text_for(run_id='0001', epochs=300, initial_word=\"Sancho\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* NLP Tokenization - [https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\n",
    "\n",
    "* Vector Representations of Words - [https://www.tensorflow.org/tutorials/word2vec#motivation_why_learn_word_embeddings](https://www.tensorflow.org/tutorials/word2vec#motivation_why_learn_word_embeddings) \n",
    "\n",
    "* Recurrent Neural Networks - [https://www.tensorflow.org/tutorials/recurrent](https://www.tensorflow.org/tutorials/recurrent) \n",
    "\n",
    "* Alex Graves - Generating Sequences With Recurrent Neural Networks [https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf)\n",
    "\n",
    "* Christopher Olah - Understanding LSTM Networks [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) \n",
    "\n",
    "* Prasad Kawthekar, Raunaq Rewari, Suvrat Bhooshan - Evaluating Generative Models for Text Generation - [https://web.stanford.edu/class/cs224n/reports/2737434.pdf](https://web.stanford.edu/class/cs224n/reports/2737434.pdf) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
